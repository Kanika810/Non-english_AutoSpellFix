# -*- coding: utf-8 -*-
"""Auto_spell_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EMcNdioNOrXEBBxiyFvPUJPLcKh7M2B8
"""

import re
import csv

# -------------------------

CANONICAL = ["ram", "aam", "namaste", "shukriya", "dhanyavad", "kaise", "kya", "acha"]
# -------------------------

# Simple Levenshtein (edit) distance
def edit_distance(a, b):
    a = a.lower()
    b = b.lower()
    n = len(a)
    m = len(b)
    # create (n+1) x (m+1) table filled with zeros
    dp = [[0] * (m + 1) for _ in range(n + 1)]
    for i in range(n + 1):
        dp[i][0] = i
    for j in range(m + 1):
        dp[0][j] = j

    for i in range(1, n + 1):
        for j in range(1, m + 1):
            if a[i-1] == b[j-1]:
                cost = 0
            else:
                cost = 1
            dp[i][j] = min(
                dp[i-1][j] + 1,      # deletion
                dp[i][j-1] + 1,      # insertion
                dp[i-1][j-1] + cost  # substitution
            )
    return dp[n][m]

# Collapse repeating letters: 'raam' -> 'ram'
def collapse_repeats(s):
    return re.sub(r'(.)\1+', r'\1', s)

# Try a few vowel replacements to handle common transliteration patterns
def vowel_variants(s):
    s = s.lower()
    variants = set([s])
    # common mappings to try
    mappings = [("au", "a"), ("o", "a"), ("u", "a")]
    for old, new in mappings:
        if old in s:
            variants.add(s.replace(old, new))
    return list(variants)

# Generate simple variants for a noisy word
def generate_simple_variants(word):
    word = word.lower()
    variants = set()
    variants.add(word)
    variants.add(collapse_repeats(word))
    for v in list(variants):
        for vv in vowel_variants(v):
            variants.add(vv)
            variants.add(collapse_repeats(vv))
    return list(variants)

# Find best match from CANONICAL; return (best_word, distance)
def find_best(word, max_distance=2):
    variants = generate_simple_variants(word)
    best_word = word
    best_dist = 999
    for var in variants:
        for cand in CANONICAL:
            d = edit_distance(var, cand)
            if d < best_dist:
                best_dist = d
                best_word = cand
            # simple tie-breaker: choose alphabetically smaller candidate (optional)
            elif d == best_dist and cand < best_word:
                best_word = cand
    if best_dist <= max_distance:
        return best_word, best_dist
    else:
        return word, best_dist

# Process input file (one token per line) and write corrections.csv
def process_file(input_path="input.txt", output_csv="corrections.csv"):
    mapping = {}  # keep unique error -> corrected
    try:
        with open(input_path, "r", encoding="utf-8") as f:
            for line in f:
                token = line.strip()
                if token == "":
                    continue
                corrected, dist = find_best(token, max_distance=2)
                # Make corrected title-case so "ram" -> "Ram" in output (optional)
                corrected_out = corrected.capitalize()
                mapping[token] = corrected_out
    except FileNotFoundError:
        print("No input file found. Create 'input.txt' with one word per line and try again.")
        return

    # write CSV
    with open(output_csv, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["File_Error", "Corrected"])
        for error_word, corr in mapping.items():
            writer.writerow([error_word, corr])

    print("Done. Processed", len(mapping), "unique tokens. See", output_csv)

# Run when executed directly
if __name__ == "__main__":
    process_file()

