# -*- coding: utf-8 -*-
"""Auto_spell_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IBLb7BlnQ8PWEtpnt4jE_QedS_Oy4IrY
"""

import re
import csv
from pathlib import Path

# -------------------------
# Config
INPUT_PATH = "input.txt"          # one token per line
REFERENCE_PATH = "reference.txt"  # dictionary with correct words, one per line
OUTPUT_CSV = "corrections.csv"
MAX_DISTANCE = 2                  # tune for precision/recall
# -------------------------

def load_reference(path):
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"Reference file not found: {path}")
    # load as lowercased unique list
    with p.open("r", encoding="utf-8") as f:
        refs = {line.strip().lower() for line in f if line.strip()}
    # return sorted list for deterministic tie-breaking
    return sorted(refs)

# collapse repeating letters: 'raam' -> 'ram'
def collapse_repeats(s):
    return re.sub(r'(.)\1+', r'\1', s)

# generate simple vowel and digraph variants
def vowel_variants(s):
    s = s.lower()
    variants = {s}
    mappings = [("au", "a"), ("aa", "a"), ("o", "a"), ("u", "a"), ("ee", "i"), ("ie", "i")]
    for old, new in mappings:
        if old in s:
            variants.add(s.replace(old, new))
    # common digraph approximations
    digraphs = [("sh", "s"), ("kh", "k"), ("gh", "g"), ("ch", "c"), ("ph", "p"), ("bh", "b")]
    for old, new in digraphs:
        if old in s:
            variants.add(s.replace(old, new))
    return variants

def generate_simple_variants(word):
    w = word.lower()
    variants = set([w, collapse_repeats(w)])
    # expand with vowel/digraph replacements
    for v in list(variants):
        for vv in vowel_variants(v):
            variants.add(vv)
            variants.add(collapse_repeats(vv))
    return variants

# efficient Levenshtein with early stop and two rows
def edit_distance_max(a, b, max_distance):
    a = a.lower()
    b = b.lower()
    if abs(len(a) - len(b)) > max_distance:
        return max_distance + 1  # too far by length filter
    # ensure a is shorter
    if len(a) > len(b):
        a, b = b, a
    previous = list(range(len(b) + 1))
    for i, ca in enumerate(a, start=1):
        current = [i] + [0] * len(b)
        # optimization: track min in row for early break
        row_min = current[0]
        for j, cb in enumerate(b, start=1):
            cost = 0 if ca == cb else 1
            deletion = previous[j] + 1
            insertion = current[j-1] + 1
            substitution = previous[j-1] + cost
            current[j] = min(deletion, insertion, substitution)
            if current[j] < row_min:
                row_min = current[j]
        if row_min > max_distance:
            return max_distance + 1
        previous = current
    dist = previous[-1]
    return dist

def find_best(word, reference_list, max_distance=2):
    variants = generate_simple_variants(word)
    best = None
    best_dist = max_distance + 1
    # cheap candidate prefilter by length
    lengths = {}
    for cand in reference_list:
        lengths.setdefault(len(cand), []).append(cand)
    # check candidate lengths within tolerance
    checked = set()
    for var in variants:
        var_len = len(var)
        for L in range(var_len - max_distance, var_len + max_distance + 1):
            if L in lengths:
                for cand in lengths[L]:
                    if (var, cand) in checked:
                        continue
                    checked.add((var, cand))
                    d = edit_distance_max(var, cand, max_distance)
                    if d < best_dist or (d == best_dist and (best is None or cand < best)):
                        best_dist = d
                        best = cand
                        if best_dist == 0:
                            return best, 0
    if best is not None and best_dist <= max_distance:
        return best, best_dist
    else:
        return None, None   # indicate no good match

def process_file(input_path=INPUT_PATH, reference_path=REFERENCE_PATH, output_csv=OUTPUT_CSV, max_distance=MAX_DISTANCE):
    refs = load_reference(reference_path)
    input_p = Path(input_path)
    if not input_p.exists():
        print(f"Input file not found: {input_path}")
        return

    rows_out = []
    with input_p.open("r", encoding="utf-8") as f:
        for line in f:
            token = line.strip()
            if not token:
                continue
            # keep original_token for output casing/punctuation
            original = token
            # attempt to correct
            best, dist = find_best(token, refs, max_distance=max_distance)
            if best is None:
                corrected = original  # no good correction found -> keep original
            else:
                # keep capitalization style: if original was uppercase -> uppercase corrected also
                if original.isupper():
                    corrected = best.upper()
                elif original[0].isupper():
                    corrected = best.capitalize()
                else:
                    corrected = best
            rows_out.append((original, corrected))

    # write CSV (preserves order, duplicates)
    with open(output_csv, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["File_Error", "Corrected"])
        for orig, corr in rows_out:
            writer.writerow([orig, corr])

    print("Done. Processed", len(rows_out), "lines. See", output_csv)


if __name__ == "__main__":
    process_file()

